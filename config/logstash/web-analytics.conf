input {

  # ATM
  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "atm/atm-apache-http/"
    add_field => {
      "[organization][name]" => "atm"
      "[url][domain]" => "pds-atmospheres.nmsu.edu"
      "[url][scheme]" => "https"
    }
    ecs_compatibility => v8
    id => "file_input_atm1"
  }
  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "atm/atm-atmos-ftp/"
    add_field => {
      "[organization][name]" => "atm"
      "[url][domain]" => "pds-atmospheres.nmsu.edu"
      "[url][scheme]" => "ftp"
    }
    ecs_compatibility => v8
    id => "file_input_atm2"
  }

  # EN
  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "en/"
    add_field => {
      "[organization][name]" => "en"
      "[url][domain]" => "pds.nasa.gov"
      "[url][scheme]" => "https"
    }
    ecs_compatibility => v8
    id => "file_input_en1"
  }

  # GEO
  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "geo/geo-an/"
    add_field => {
      "[organization][name]" => "geo"
      "[url][domain]" => "an.rsl.wustl.edu"
      "[url][scheme]" => "https"
    }
    ecs_compatibility => v8
    id => "file_input_geo1"
  }

  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "geo/geo-ode/"
    add_field => {
      "[organization][name]" => "geo"
      "[url][domain]" => "ode.rsl.wustl.edu"
      "[url][scheme]" => "https"
    }
    ecs_compatibility => v8
    id => "file_input_geo2"
  }

  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "geo/geo-ode"
    add_field => {
      "[organization][name]" => "geo"
      "[url][domain]" => "ode.rsl.wustl.edu"
      "[url][scheme]" => "https"
    }
    ecs_compatibility => v8
    id => "file_input_geo3"
  }

  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "geo/geo-pds"
    add_field => {
      "[organization][name]" => "geo"
      "[url][domain]" => "pds-geosciences.wustl.edu"
      "[url][scheme]" => "https"
    }
    ecs_compatibility => v8
    id => "file_input_geo4"
  }

  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "geo/geo-speclib"
    add_field => {
      "[organization][name]" => "geo"
      "[url][domain]" => "pds-speclib.rsl.wustl.edu"
      "[url][scheme]" => "https"
    }
    ecs_compatibility => v8
    id => "file_input_geo5"
  }

  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "geo/geo-va"
    add_field => {
      "[organization][name]" => "geo"
      "[url][domain]" => "pds-geosciences.wustl.edu"
      "[url][scheme]" => "https"
    }
    ecs_compatibility => v8
    id => "file_input_geo6"
  }

  # NAIF
  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "naif/"
    add_field => {
      "[organization][name]" => "naif"
      "[url][domain]" => "naif.jpl.nasa.gov"
      "[url][scheme]" => "https"
    }
    ecs_compatibility => v8
    id => "file_input_naif1"
  }

  # PPI
  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "img/"
    add_field => {
      "[organization][name]" => "img"
      "[url][domain]" => "pds-imaging.jpl.nasa.gov"
      "[url][scheme]" => "https"
    }
    ecs_compatibility => v8
    id => "file_input_img1"
  }

  # PPI
  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "ppi/"
    add_field => {
      "[organization][name]" => "ppi"
      "[url][domain]" => "pds-ppi.igpp.ucla.edu"
      "[url][scheme]" => "https"
    }
    ecs_compatibility => v8
    id => "file_input_ppi1"
  }

  # SBN
  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "sbn/sbn-apache-http/"
    add_field => {
      "[organization][name]" => "sbn"
      "[url][domain]" => "pds-smallbodies.astro.umd.edu"
      "[url][scheme]" => "https"
    }
    ecs_compatibility => v8
    id => "file_input_sbnumd1"
  }

  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "sbn/sbn-psi-tomcat/"
    add_field => {
      "[organization][name]" => "sbn"
      "[url][domain]" => "sbn.psi.edu"
      "[url][scheme]" => "https"
    }
    ecs_compatibility => v8
    id => "file_input_sbnpsi1"
  }

  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "sbn/sbn-psi-tools/"
    add_field => {
      "[organization][name]" => "sbn"
      "[url][domain]" => "sbnapps.psi.edu"
      "[url][scheme]" => "https"
    }
    ecs_compatibility => v8
    id => "file_input_sbnpsi2"
  }

}

filter {

  # Currently, @timestamp is set to the current time.  We store that in the
  # index_time field so that we can tell when records are ingested.  Later,
  # @timestamp will be overwritten with the timestamp of the logged request.
  mutate {
    add_field => { "index_time" => "%{@timestamp}" }
  }

  # Parse the logged request.  We assume that all logs have been converted into
  # Apache/Combined format beforehand.
  grok {
    ecs_compatibility => "v8"
    match => { "message" => [
          "%{COMBINEDAPACHELOG}",
          "%{COMMONAPACHELOG}",
          "%{HOSTNAME:[source][address]} - - \[%{HTTPDATE:timestamp}\] \"%{WORD:[http][request][method} %{DATA:[url][original]} HTTP/%{NUMBER:[http][version]}\"%{NUMBER:[http][response][status_code]} %{NUMBER:[http][response][body][bytes]}"
                ]
          }
    id => "grok_filter_initial_parse"
    tag_on_failure => " "
  }

  # Use uap_core regex patterns to detect bots/spiders
  useragent {
    ecs_compatibility => "v8"
    source => "[user_agent][original]"
    regexes => "/home/ssm-user/regexes.yaml"
    id => "useragent_filter_parse_apache_combined"
  }

  # Drop internal logs
  ## Drop EN ELB healthcheck logs or logs that start with .
  if ("ELB-HealthChecker/2.0" in [message] or [message] =~ /^\..*/) {
    ruby {
      code => 'puts "Dropping event: #{event.to_hash}"'
    }
    drop { }
  }

  # Determine the geographic location from which the request came.
  if ![tags] {
     geoip {
         ecs_compatibility => "v8"
         source => "[source][address]"
         target => "source"
     }
  }

  # Reverse lookup any raw IP addresses to get the domain from which the
  # request originated.
  #if ![tags] {
    mutate {
      add_field => { "[source][domain]" => "%{[source][address]}" }
    }
    #grok {
    #  match => { "clientdomain" => "%{IP}" }
    #  add_tag => [ "_clientip_lookup_failure" ]
    #  tag_on_failure => []
    #  id => "grok_filter_raw_ip_check"
    #}
    #if "_clientip_lookup_failure" in [tags] {
    if [source][address] =~ /^\d{1,3}(\.\d{1,3}){3}$/ {
      dns {
        action => "replace"
        reverse => [ "[source][domain]" ]
        nameserver => [ "10.1.0.2" ]
        add_tag => [ "_dns_reverse_lookup" ]
        hit_cache_size => 10000
        failed_cache_size => 1000
        failed_cache_ttl => 60
        max_retries => 5
      }
    } else {
      dns {
        action => "replace"
        resolve => [ "[source][address]" ]
        nameserver => [ "10.1.0.2" ]
        add_tag => [ "_dns_resolve_lookup" ]
        hit_cache_size => 10000
        failed_cache_size => 1000
        failed_cache_ttl => 60
        max_retries => 5
      }
    }

    # Parse URL
    grok {
      ecs_compatibility => "v8"
      match => {
        "[url][original]" => [
          # match https://user:pwd@stuff.domain.com:8080/some/path?p1=v1&p2=v2#anchor
          "%{URIPROTO:[url][scheme]}://(?:%{USER:[url][username]}:(?<[url][password]>[^@]*)@)?(?:%{IPORHOST:[url][address]}(?::%{POSINT:[url][port]}))?(?:%{URIPATH:[url][path]}(?:%{URIPARAM:[url][query]}))?",
          # match stuff.domain.com:8080/some/path?p1=v1&p2=v2#anchor
          "%{IPORHOST:[url][address]}(?::%{POSINT:[url][port]})(?:%{URIPATH:[url][path]}(?:%{URIPARAM:[url][query]}))?",
          # match /some/path?p1=v1&p2=v2#anchor
          "%{URIPATH:[url][path]}(?:%{URIPARAM:[url][query]})"
        ]
      }
      add_tag => [ "_url_contains_query" ]
      tag_on_failure => "_grokparsefailure_querymatch"
    }
  if "_url_contains_query" in [tags] {
    # parse the address to distinguish domain or ip
    grok {
      match => {
        "[url][address]" => "(%{IP:[source][address]}|%{HOSTNAME:[url][domain]})"
      }
      tag_on_failure => "_grokparsefailure_ipvsdomain"
    }
    # Requires a custom plugin here, see https://www.elastic.co/guide/en/logstash/current/plugins-filters-tld.html
    tld {
      source => "[url][domain]"
      target => "[url][tld]"
    }
    mutate {
      rename => {
        "[url][tld][domain]" => "[url][registered_domain]"
        "[url][tld][tld]" => "[url][top_level_domain]"
        "[url][tld][sld]" => "[url][second_level_domain]"
        "[url][tld][trd]" => "[url][sub_domain]"
      }
      remove_field => [ "[url][tld]" ]
    }
    # parse the query to extract fragment
    grok {
      match => {
        "[url][query]" => "^\?(?<[url][query]>[A-Za-z0-9$.+!*'|(){},~@%&/=:;_?\-\[\]<>]*)(?:#(?:%{WORD:[url][fragment]}))?"
      }
      overwrite => [ "[url][query]" ]
      tag_on_failure => "_grokparsefailure_urlquery"
    }
    kv {
      source => "[url][query]"
      field_split => "&"
      value_split => "="
      target => "[url][queryparams]"
    }
    urldecode {
      field => "[url][query]"
    }
    urldecode {
      field => "[url][queryparams]"
    }
  } else {
    mutate {
      add_field => { "[url][path]" => "%{[url][original]}" }
    }
  }
    #}
 # }

  # Additional filtering for spiders
  #if [device] != "Spider" and [clientdomain]{
  #  grok {
  #    patterns_dir => ["./patterns"]
  #    match => { "clientdomain" => "%{BOTS}" }
  #    tag_on_failure => []
  #    add_tag => ["_its_a_bot"]
  #    id => "grok_filter_domain_spiders"
  #  }
  #  if "_its_a_bot" in [tags]{
  #    mutate {
  #      update => { "device" => "Spider" }
  #      remove_tag => [ "_its_a_bot" ]
  #    }
  #  }
  #}

  # Get the client's top level domain.
  #tld {
  #  source => "[source][domain]"
  #}

  # I *think* that this is where we format the timestamp to Kibana's preferred format.
  date {
    match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
  }

  # Use the raw event message as a UUID to avoid duplication in elasticsearch
  fingerprint {
    ecs_compatibility => "v8"
    source => "message"
    target => "[event][hash]"
    method => "MURMUR3"
  }

  # Finally, remove the tags field if it is empty to show that the event is good
  # to ingest
  #if [tags] == "" {
  #  mutate {
  #    remove_field => ["tags"]
  #  }
  #}
}

output {
#    if [tags] {
        # Print the event details if any tags are present
#        stdout { codec => "rubydebug" }
#    } #else {
        # If the event has no tags, simply print a dot to help track throughput
      #  stdout { codec => line { format => "." } }
   # }
    opensearch {
        ecs_compatibility => "v8"
        index => "en-web-analytics"
        hosts => "https://p5qmxrldysl1gy759hqf.us-west-2.aoss.amazonaws.com:443"
        auth_type => {
            type => 'aws_iam'
            aws_access_key_id => ''
            aws_secret_access_key => ''
            region => 'us-west-2'
            service_name => 'aoss'
        }
        legacy_template => false
        default_server_major_version => 2
        document_id => "%{[event][hash]}"
        manage_template => false
    }
}