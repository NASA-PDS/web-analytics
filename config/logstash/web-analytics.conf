input {
#  generator {
#    lines => [
#    '. - - [14/Dec/2024:11:09:27 -0500] "GET /holdings/nh-a-leisa-3-kem1-v1.0/dataset.shtml HTTP/1.1" 200 18000 "-" "Opera/9.18.(Windows NT 5.01; th-TH) Presto/2.9.173 Version/12.00"'
#     '10.100.1.106 - - [02/May/2024:17:02:54 -0700] "GET / HTTP/1.1" 200 44153 "-" "ELB-HealthChecker/2.0"',
#     '142.250.189.206 - - [02/May/2024:17:16:56 -0700] "GET /data/pvo-v-pos-5--vsocoords-12sec-v1.0/pv01_0010/data/omag/hires/orb0440.ffh HTTP/1.1" 200 1728 "-" "Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm) Chrome/116.0.1938.76 Safari/537.36"',
#     '66.249.66.1 - - [30/Apr/2024:17:00:04 -0700] "GET /services/search/search?q=&f.facet_instrument.facet.prefix=3%2Cparticle%20detector%2Ccospin-low%20energy%20telescope%2C&fq=facet_target%3A%223%2Csatellite%2Cjupiter%20moon%2Cthebe%22&f.facet_target.facet.prefix=4%2Csatellite%2Cjupiter%20moon%2Cthebe%2C&f.facet_type.facet.prefix=2%2Ctarget%2C&fq=facet_investigation%3A%221%2Cgalileo%22&f.facet_investigation.facet.prefix=2%2Cgalileo%2C HTTP/1.1" 200 7017 "-" "Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; GPTBot/1.0; +https://openai.com/gptbot)"',
#     'dc-vlan1725.nmsu.edu - - [04/Nov/2024:01:33:18 -0700] "GET /data_and_services/atmospheres_data/JUNO/jiram/data_raw/orbit37/JIR_LOG_IMG_EDR_2021289T201105_V01.LBL HTTP/1.1"404 196'
#      '92.119.179.83 - ${${ [03/Jan/2024:01:03:44 -0800] "GET /roses/ HTTP/1.1" 200 810 "-" "python-requests/2.31.0"'
#      '213.152.161.5 ? - [12/Jan/2025:09:03:29 -0800] "GET /robots.txt HTTP/1.1" 200 352 "-" "-"'
#    ]
#    count => 1
#  }

  # ATM
  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "atm/atm-apache-http/"
    add_field => {
      "[organization][name]" => "atm"
      "[url][domain]" => "pds-atmospheres.nmsu.edu"
      "[url][scheme]" => "https"
    }
    ecs_compatibility => v8
    id => "file_input_atm1"
  }
  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "atm/atm-atmos-ftp/"
    add_field => {
      "[organization][name]" => "atm"
      "[url][domain]" => "pds-atmospheres.nmsu.edu"
      "[url][scheme]" => "ftp"
    }
    ecs_compatibility => v8
    id => "file_input_atm2"
  }

  # EN
  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "en/"
    add_field => {
      "[organization][name]" => "en"
      "[url][domain]" => "pds.nasa.gov"
      "[url][scheme]" => "https"
    }
    ecs_compatibility => v8
    id => "file_input_en1"
  }

  # GEO
  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "geo/geo-an/"
    add_field => {
      "[organization][name]" => "geo"
      "[url][domain]" => "an.rsl.wustl.edu"
      "[url][scheme]" => "https"
    }
    ecs_compatibility => v8
    id => "file_input_geo1"
  }

  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "geo/geo-ode/"
    add_field => {
      "[organization][name]" => "geo"
      "[url][domain]" => "ode.rsl.wustl.edu"
      "[url][scheme]" => "https"
    }
    ecs_compatibility => v8
    id => "file_input_geo2"
  }

  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "geo/geo-ode"
    add_field => {
      "[organization][name]" => "geo"
      "[url][domain]" => "ode.rsl.wustl.edu"
      "[url][scheme]" => "https"
    }
    ecs_compatibility => v8
    id => "file_input_geo3"
  }

  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "geo/geo-pds"
    add_field => {
      "[organization][name]" => "geo"
      "[url][domain]" => "pds-geosciences.wustl.edu"
      "[url][scheme]" => "https"
    }
    ecs_compatibility => v8
    id => "file_input_geo4"
  }

  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "geo/geo-speclib"
    add_field => {
      "[organization][name]" => "geo"
      "[url][domain]" => "pds-speclib.rsl.wustl.edu"
      "[url][scheme]" => "https"
    }
    ecs_compatibility => v8
    id => "file_input_geo5"
  }

  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "geo/geo-va"
    add_field => {
      "[organization][name]" => "geo"
      "[url][domain]" => "pds-geosciences.wustl.edu"
      "[url][scheme]" => "https"
    }
    ecs_compatibility => v8
    id => "file_input_geo6"
  }

  # NAIF
  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "naif/"
    add_field => {
      "[organization][name]" => "naif"
      "[url][domain]" => "naif.jpl.nasa.gov"
      "[url][scheme]" => "https"
    }
    ecs_compatibility => v8
    id => "file_input_naif1"
  }

  # PPI
  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "img/"
    add_field => {
      "[organization][name]" => "img"
      "[url][domain]" => "pds-imaging.jpl.nasa.gov"
      "[url][scheme]" => "https"
    }
    ecs_compatibility => v8
    id => "file_input_img1"
  }

  # PPI
  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "ppi/"
    add_field => {
      "[organization][name]" => "ppi"
      "[url][domain]" => "pds-ppi.igpp.ucla.edu"
      "[url][scheme]" => "https"
    }
    ecs_compatibility => v8
    id => "file_input_ppi1"
  }

  # SBN
  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "sbn/sbn-apache-http/"
    add_field => {
      "[organization][name]" => "sbn"
      "[url][domain]" => "pds-smallbodies.astro.umd.edu"
      "[url][scheme]" => "https"
    }
    ecs_compatibility => v8
    id => "file_input_sbnumd1"
  }

  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "sbn/sbn-psi-tomcat/"
    add_field => {
      "[organization][name]" => "sbn"
      "[url][domain]" => "sbn.psi.edu"
      "[url][scheme]" => "https"
    }
    ecs_compatibility => v8
    id => "file_input_sbnpsi1"
  }

  s3 {
    bucket => "pds-dev-web-analytics"
    region => "us-west-2"
    prefix => "sbn/sbn-psi-tools/"
    add_field => {
      "[organization][name]" => "sbn"
      "[url][domain]" => "sbnapps.psi.edu"
      "[url][scheme]" => "https"
    }
    ecs_compatibility => v8
    id => "file_input_sbnpsi2"
>>>>>>> fb79792 (Updates to support ATM and PPI log oddities)
  }

}

filter {

  # Currently, @timestamp is set to the current time.  We store that in the
  # index_time field so that we can tell when records are ingested.  Later,
  # @timestamp will be overwritten with the timestamp of the logged request.
  mutate {
    add_field => { "[event][ingested]" => "%{@timestamp}" }
  }

  # Parse the logged request.  We assume that all logs have been converted into
  # Apache/Combined format beforehand.
  grok {
    ecs_compatibility => "v8"
    match => { "message" => [
        "%{COMBINEDAPACHELOG}",
        "%{COMMONAPACHELOG}",

        # Handle ATM logs where the IP is already translated to a host
        "%{HOSTNAME:[source][address]} - - \[%{HTTPDATE:@timestamp}\] \"%{WORD:[http][request][method]} %{DATA:[url][original]} HTTP/%{NUMBER:[http][version]}\"%{NUMBER:[http][response][status_code]} %{NUMBER:[http][response][body][bytes]}",

        # Handle PPI and ATM logs where username info is wonky
        "%{IP:[source][address]} %{NOTSPACE:[user][ident]} %{NOTSPACE:[user][name]} \[%{HTTPDATE:@timestamp}\] \"%{WORD:[http][request][method]} %{URIPATH:[url][original]}(?: HTTP\/%{NUMBER:[http][version]})?\" %{NUMBER:[http][response][status_code]} %{NUMBER:[http][response][body][bytes]} \"%{DATA:[http][request][referrer]}\" \"%{DATA:[user_agent][original]}\""
      ]
    }
    id => "grok_filter_initial_parse"
    tag_on_failure => " "
  }

  # Use uap_core regex patterns to detect bots/spiders
  useragent {
    ecs_compatibility => "v8"
    source => "[user_agent][original]"
    regexes => "/Users/jpadams/proj/pds/pdsen/workspace/web-analytics/config/logstash/regexes.yaml"
    id => "useragent_filter_parse_apache_combined"
  }

  # Drop internal logs
  ## Drop EN ELB healthcheck logs or logs that start with .
  if ("ELB-HealthChecker/2.0" in [message] or [message] =~ /^\..*/) {
    ruby {
      code => 'puts "Dropping event: #{event.to_hash}"'
    }
    drop { }
  }

  # Determine the geographic location from which the request came.
  if ![tags] {
     geoip {
         ecs_compatibility => "v8"
         source => "[source][address]"
         target => "source"
     }
  }

  # Reverse lookup any raw IP addresses to get the domain from which the
  # request originated.
  #if ![tags] {
    mutate {
      add_field => { "[source][domain]" => "%{[source][address]}" }
    }
    #grok {
    #  match => { "clientdomain" => "%{IP}" }
    #  add_tag => [ "_clientip_lookup_failure" ]
    #  tag_on_failure => []
    #  id => "grok_filter_raw_ip_check"
    #}
    #if "_clientip_lookup_failure" in [tags] {
    if [source][address] =~ /^\d{1,3}(\.\d{1,3}){3}$/ {
      dns {
        action => "replace"
        reverse => [ "[source][domain]" ]
        nameserver => [ "10.1.0.2" ]
        add_tag => [ "_dns_reverse_lookup" ]
        hit_cache_size => 10000
        failed_cache_size => 1000
        failed_cache_ttl => 60
        max_retries => 5
      }
    } else {
      dns {
        action => "replace"
        resolve => [ "[source][address]" ]
        nameserver => [ "10.1.0.2" ]
        add_tag => [ "_dns_resolve_lookup" ]
        hit_cache_size => 10000
        failed_cache_size => 1000
        failed_cache_ttl => 60
        max_retries => 5
      }
    }

    # Parse URL
    grok {
      ecs_compatibility => "v8"
      match => {
        "[url][original]" => [
          # match https://user:pwd@stuff.domain.com:8080/some/path?p1=v1&p2=v2#anchor
          "%{URIPROTO:[url][scheme]}://(?:%{USER:[url][username]}:(?<[url][password]>[^@]*)@)?(?:%{IPORHOST:[url][address]}(?::%{POSINT:[url][port]}))?(?:%{URIPATH:[url][path]}(?:%{URIPARAM:[url][query]}))?",
          # match stuff.domain.com:8080/some/path?p1=v1&p2=v2#anchor
          "%{IPORHOST:[url][address]}(?::%{POSINT:[url][port]})(?:%{URIPATH:[url][path]}(?:%{URIPARAM:[url][query]}))?",
          # match /some/path?p1=v1&p2=v2#anchor
          "%{URIPATH:[url][path]}(?:%{URIPARAM:[url][query]})"
        ]
      }
      add_tag => [ "_url_contains_query" ]
      tag_on_failure => "_grokparsefailure_querymatch"
    }
  if "_url_contains_query" in [tags] {
    # parse the address to distinguish domain or ip
    grok {
      match => {
        "[url][address]" => "(%{IP:[source][address]}|%{HOSTNAME:[url][domain]})"
      }
      tag_on_failure => "_grokparsefailure_ipvsdomain"
    }
    # Requires a custom plugin here, see https://www.elastic.co/guide/en/logstash/current/plugins-filters-tld.html
    tld {
      source => "[url][domain]"
      target => "[url][tld]"
    }
    mutate {
      rename => {
        "[url][tld][domain]" => "[url][registered_domain]"
        "[url][tld][tld]" => "[url][top_level_domain]"
        "[url][tld][sld]" => "[url][second_level_domain]"
        "[url][tld][trd]" => "[url][sub_domain]"
      }
      remove_field => [ "[url][tld]" ]
    }
    # parse the query to extract fragment
    grok {
      match => {
        "[url][query]" => "^\?(?<[url][query]>[A-Za-z0-9$.+!*'|(){},~@%&/=:;_?\-\[\]<>]*)(?:#(?:%{WORD:[url][fragment]}))?"
      }
      overwrite => [ "[url][query]" ]
      tag_on_failure => "_grokparsefailure_urlquery"
    }
    kv {
      source => "[url][query]"
      field_split => "&"
      value_split => "="
      target => "[url][queryparams]"
    }
    urldecode {
      field => "[url][query]"
    }
    urldecode {
      field => "[url][queryparams]"
    }
  } else {
    mutate {
      add_field => { "[url][path]" => "%{[url][original]}" }
    }
  }
    #}
 # }

  # Additional filtering for spiders
  #if [device] != "Spider" and [clientdomain]{
  #  grok {
  #    patterns_dir => ["./patterns"]
  #    match => { "clientdomain" => "%{BOTS}" }
  #    tag_on_failure => []
  #    add_tag => ["_its_a_bot"]
  #    id => "grok_filter_domain_spiders"
  #  }
  #  if "_its_a_bot" in [tags]{
  #    mutate {
  #      update => { "device" => "Spider" }
  #      remove_tag => [ "_its_a_bot" ]
  #    }
  #  }
  #}

  # Get the client's top level domain.
  #tld {
  #  source => "[source][domain]"
  #}

  # I *think* that this is where we format the timestamp to Kibana's preferred format.
  date {
    match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
  }

  # Use the raw event message as a UUID to avoid duplication in elasticsearch
  fingerprint {
    ecs_compatibility => "v8"
    source => "message"
    target => "[event][hash]"
    method => "MURMUR3"
  }

  # Finally, remove the tags field if it is empty to show that the event is good
  # to ingest
  #if [tags] == "" {
  #  mutate {
  #    remove_field => ["tags"]
  #  }
  #}
}

output {
    stdout { codec => "rubydebug" }
#    if [tags] {
        # Print the event details if any tags are present
#        stdout { codec => "rubydebug" }
#    } #else {
        # If the event has no tags, simply print a dot to help track throughput
      #  stdout { codec => line { format => "." } }
   # }

}